{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# üì¶ INSTALL LIBRARIES\n",
        "!pip install -q gradio\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLB4cTMiyA9I",
        "outputId": "79da2880-af13-444e-9c0e-3aa0e1204c25"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üì• IMPORTS\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n"
      ],
      "metadata": {
        "id": "CySmDf82yEhV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß LOAD MODEL AND TOKENIZER\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH1IVxUMyLcc",
        "outputId": "8181d03d-9337-4ba2-fe94-1a7e540b12d3"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† DEFINE TEXT GENERATION FUNCTION\n",
        "def generate_text(input):\n",
        "    input_ids = tokenizer.encode(input, return_tensors=\"tf\")\n",
        "    beam_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return \".\".join(output.split(\". \")[:-1]) + \".\"\n"
      ],
      "metadata": {
        "id": "sKgqft2HyPy8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üñ•Ô∏è CREATE INTERFACE\n",
        "output_text = gr.Textbox()\n",
        "interface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=\"textbox\",\n",
        "    outputs=output_text,\n",
        "    title=\"GPT-2 Text Generator\",\n",
        "    description=\"Give it a sentence and GPT-2 will complete it. Based on OpenAI's transformer model.\"\n",
        ")\n",
        "\n",
        "# üöÄ LAUNCH APP\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "b6vMvqG-yTII",
        "outputId": "0fc30f62-70b0-4fe0-f74b-1ea9404eaeaa"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fc8678e244ba34a51c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fc8678e244ba34a51c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def advanced_generate_text(prompt, max_length, temperature, top_k):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"tf\")\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=int(max_length),\n",
        "        temperature=float(temperature),\n",
        "        top_k=int(top_k),\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return decoded\n",
        "\n",
        "# New Interface with sliders\n",
        "interface = gr.Interface(\n",
        "    fn=advanced_generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Prompt\"),\n",
        "        gr.Slider(50, 300, value=100, step=10, label=\"Max Length\"),\n",
        "        gr.Slider(0.1, 1.5, value=1.0, step=0.1, label=\"Temperature\"),\n",
        "        gr.Slider(0, 100, value=50, step=5, label=\"Top-k Sampling\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"üß† Advanced GPT-2 Generator\",\n",
        "    description=\"Adjust the generation parameters for more control.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "diHLwikvz0So",
        "outputId": "01f09452-f86f-4963-c233-a8c986b67319"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://59fcf51371e7af48d9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://59fcf51371e7af48d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This saves your GPT-2 app code into app.py\n",
        "app_code = '''\n",
        "import gradio as gr\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "def generate_text(input):\n",
        "    input_ids = tokenizer.encode(input, return_tensors=\"tf\")\n",
        "    beam_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "    return \".\".join(output.split(\".\")[:-1]) + \".\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=\"textbox\",\n",
        "    outputs=\"textbox\",\n",
        "    title=\"GPT-2 Text Generator\",\n",
        "    description=\"Enter some text and GPT-2 will generate continuation.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# Save requirements.txt file\n",
        "requirements = '''\n",
        "gradio\n",
        "tensorflow\n",
        "transformers\n",
        "'''\n",
        "\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n",
        "\n",
        "# Save README.md file\n",
        "readme = '''\n",
        "# GPT-2 Text Generator App\n",
        "\n",
        "This app uses GPT-2 and Gradio to generate text continuations.\n",
        "\n",
        "Run with:\n",
        "\n",
        "    pip install -r requirements.txt\n",
        "    python app.py\n",
        "'''\n",
        "\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme)\n"
      ],
      "metadata": {
        "id": "eYxVmovjZHE6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPx99vC4bV_l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}